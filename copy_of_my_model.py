# -*- coding: utf-8 -*-
"""Copy of My_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oQUfub9-4ixuxTyW_K2FwpSCwVhnNM7v
"""

import pandas as pd
import numpy as np
import torch
from sklearn.preprocessing import StandardScaler

def load_time_series(csv_path):
    df = pd.read_csv(csv_path)

    # Convert timestamp to datetime
    df['date'] = pd.to_datetime(df['timestamp'])

    # Time-based cyclical features
    df['hour_sin'] = np.sin(2 * np.pi * df['date'].dt.hour / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['date'].dt.hour / 24)
    df['dow_sin'] = np.sin(2 * np.pi * df['date'].dt.dayofweek / 7)
    df['dow_cos'] = np.cos(2 * np.pi * df['date'].dt.dayofweek / 7)

    # Drop timestamp and original date column
    df = df.drop(columns=['sim_time','timestamp', 'date'])

    # Detect target columns that start with "Gate"
    target_cols = [col for col in df.columns if col.startswith("Gate")]

    # Reorder columns: all non-targets first, then targets at the end
    df = df[[col for col in df.columns if col not in target_cols] + target_cols]

    print(f"All columns: {df.columns.tolist()}")
    print(f"Detected target columns: {target_cols}")
    return df, df.shape[1]

def split_dataframe(df, train_ratio=0.8):
    split_idx = int(len(df) * train_ratio)
    return df[:split_idx], df[split_idx:]

def prepare_data(df, scaler=None, fit_scaler=False):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    # Detect features and targets dynamically
    feature_columns = [
        col for col in df.columns
        if col.startswith("PIR")
        or col.startswith("Vibration")
        or col.startswith("Sound")
        or col in ["hour_sin", "hour_cos", "dow_sin", "dow_cos"]
    ]

    target_columns = [col for col in df.columns if col.startswith("Gate")]

    print(f"Detected {len(feature_columns)} feature columns.")
    print(f"Detected {len(target_columns)} target columns.")

    # Extract features and targets as NumPy arrays
    features = df[feature_columns].values
    targets = df[target_columns].values
    pos_weights = []
    # Print class distribution for each gate (optional)
    for idx, gate in enumerate(target_columns):
        num_positives = (targets[:, idx] == 1).sum()
        num_negatives = (targets[:, idx] == 0).sum()
        pos_weight = num_negatives / num_positives
        pos_weights.append(pos_weight)
        print(f"{gate} -> Class 0: {num_negatives}, Class 1: {num_positives}")


    pos_weights_tensor = torch.tensor(pos_weights, dtype=torch.float32).to(device)
    print("Pos weight tensor:", pos_weights_tensor)

    # Scale only the features
    if fit_scaler:
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
    else:
        features_scaled = scaler.transform(features)

    # Combine scaled features with targets
    full_data = np.hstack([features_scaled, targets])
    data_tensor = torch.tensor(full_data, dtype=torch.float32)

    return data_tensor, scaler, pos_weights_tensor


def create_dataset(data, context_len, target_len):
    x, y = [], []
    max_idx = len(data) - context_len - target_len
    for i in range(max_idx):
        x.append(data[i : i + context_len])
        y.append(data[i + context_len : i + context_len + target_len])
    x = torch.stack(x)  # shape: [N, context_len, features]
    y = torch.stack(y)  # shape: [N, target_len, features]
    return x, y


csv_path = ("/sensor_data.feather")
data, num_features = load_time_series("/sensor_data.feather")

#Defining context, target and batch_size
#How past velues example 60s
context_len = 60

# 30s,60s,120s in future
target_len = 30

# Making batches to send the data in transformer , we send data in batches of size 32
batch_size = 32      #-----------KEEP THIS 32----------

train_data ,val_data = split_dataframe(data, 0.8)
print("train_data shape: ",train_data.shape)
print("val_data shape: ",val_data.shape)

train_data_raw = train_data.copy() #-------Now i can apply scaling without losing data HAHAHA
val_data_raw = val_data.copy()

train_data ,scaler, pos_weights_tensor = prepare_data(train_data_raw, fit_scaler=True)
val_data ,scaler ,_= prepare_data(val_data_raw, scaler=scaler, fit_scaler=False)
print(type(data))
print(data.shape)
print(data.iloc[:, -1])


def get_batch(split):



    #checking what type of the data it is
    data = train_data if split=="train" else val_data

    # if split == "train":
    #   data = normlisation(data)
    # else :
    #   mean = torch.tensor(np.load("mean.npy"), dtype=torch.float32)
    #   std = torch.tensor(np.load("std.npy"), dtype=torch.float32)
    #   data = (data - mean) / std

    # this prevents slicer to go past the length of data
    max_idx = len(data) - context_len - target_len

    #This picks three random starting values from data
    ix = torch.randint(0, max_idx, (batch_size,)) # batch_size number ke columns ke random starting points
    # features = []
    # target = []
    # for i in ix:
    #   features =
    #   target =
    #   print(f"i: {i} ,features: {features.shape}, target: {target.shape}")

    # print(f"features: {data[0:0+context_len,:8]}")
    # print(f"target :{data[0:0+context_len+target_len,-1:]}")
    #defining context and target batch (input and outputs)
    # x = torch.stack([data[i : i+context_len,:112] for i in ix]) #(shape : [batch_size , context_len, num_features])

    # #target batch
    # y = torch.stack([data[i + context_len : i + context_len + target_len, -9:] for i in ix]) #(shape : [batch_size , target_len, num_features])
    num_features = data.shape[1] - 9
    x = torch.stack([data[i : i+context_len, :num_features] for i in ix])
    y = torch.stack([data[i+context_len : i+context_len+target_len, -9:] for i in ix])
    # print("shape of y: ",y.shape)
    return x,y


x_batch, y_batch = get_batch("train")
print("x_batch.shape:", x_batch.shape)
print("y_batch.shape:", y_batch.shape)
print("Sample x_batch[0]:", x_batch[0])
print("Sample y_batch[0]:", y_batch[0])





import torch
import torch.nn as nn
import torch.nn.functional as F

class TimeSeriesTransformer(nn.Module):
    def __init__(self, num_features=112, d_model=64, nhead=4, num_layers=2, dropout=0.5,
                 context_len=60, target_len=60):
        super().__init__() #d_model is the dimension in which our input is projected for the computation

        self.context_len = context_len
        self.target_len = target_len
        self.num_features = num_features
        self.d_model = d_model
        self.final_norm = nn.LayerNorm(d_model)

        # Project input features to d_model
        self.input_proj = nn.Linear(num_features, d_model)  #This projects our (batch_size, context_len, num_features) into [batch_size, context_len, d_model]

        # Learnable positional embeddings (context_len positions)
        self.pos_embedding = nn.Parameter(torch.randn(1, context_len, d_model))

        # Transformer encoder
        self.encoder_layer = nn.ModuleList([
            CustomTransformerEncoderLayer(d_model, nhead, dropout) for _ in range(num_layers)
        ])

        # Output projection from d_model to num_features
        self.output_proj = nn.Linear(d_model, 9*target_len) #output projection shape [batch_size, target_len, num_features] which is the real shape of our data

    def forward(self, x):  # x shape: [B, context_len, num_features]
        B, T, F = x.shape

        # Step 1: Project input to d_model
        x = self.input_proj(x)               # [B, context_len, d_model]

        # Step 2: Add positional encoding
        x = x + self.pos_embedding[:, :T, :] # [B, context_len, d_model] gives sense of position to our model

        #adding the loop over encoder layer because we can not use transformer encoder
        for layer in self.encoder_layer:
            x = layer(x)

        # # Step 3: Pass through transformer
        # x = self.transformer(x)              # [B, context_len, d_model] passing through transfomer stack

        # Step 4: Take only the final token's output (or mean of all tokens)
        # summary = self.final_norm(x[:, -1, :])  # [B, d_model]
        #     # [B, d_model]  (summary vector) only final token output is considered because it the summary of the input embeddigns which can not be true at sometimes
        summary = self.final_norm(x.mean(dim=1))  # [B, d_model] #now we are using mean pool of all the projectin outputs

        # Step 5: Predict future time steps
        out = self.output_proj(summary)      # [B, target_len * num_features] projecting the prediction to the output
        out = out.view(B, self.target_len, 9)  # [B, target_len, num_features] # making the dimensions right

        return out

import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"

        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads  # Size of each head

        # Learnable projections for Q, K, V
        self.query = nn.Linear(d_model, d_model)
        self.key   = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)

        # Final linear projection after concatenating heads
        self.out_proj = nn.Linear(d_model, d_model)

    def forward(self, x):
        B, T, C = x.shape  # [Batch, Time, Channels]

        # Project inputs to Q, K, V
        Q = self.query(x)  # [B, T, d_model]
        K = self.key(x)
        V = self.value(x)

        # Split into multiple heads
        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # [B, num_heads, T, head_dim]
        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)

        # Scaled dot-product attention
        scores = Q @ K.transpose(-2, -1) / (self.head_dim ** 0.5)  # [B, num_heads, T, T]
        # attn_weights = F.softmax(scores, dim=-1)           #----------------DO NOT SIGMOID IN BCEWITHLOGITSLOSS---------
        out = scores @ V  # [B, num_heads, T, head_dim]

        # Concatenate heads
        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)  # [B, T, d_model]

        # Final projection
        return self.out_proj(out)

class CustomTransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.5):
        super().__init__()
        self.attn = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.ReLU(),
            nn.Linear(4 * d_model, d_model),
            nn.Dropout(dropout)
        )
        self.norm2 = nn.LayerNorm(d_model)
        ''' Some important things : in this forward method x is added at steps , not in one go because for the optimiser to work efficiently  we
            leave some connections to go into , at each step optimiser can one optimise each one ,in simpler words it gives path to optimiser to
          to each step of transformer efficiently . These are called RESIDUAL CONNECTIONS
        '''

    def forward(self, x):
        # Self-attention + residual + norm
        attn_out = self.attn(x)
        x = self.norm1(x + attn_out)

        # Feedforward + residual + norm
        ff_out = self.ff(x)
        x = self.norm2(x + ff_out)

        return x

def binary_accuracy(y_pred, y_true, threshold=0.5):
    """
    y_pred: [B, T, num_gates] raw logits
    y_true: [B, T, num_gates] binary labels
    """
    with torch.no_grad():
        probs = torch.sigmoid(y_pred)
        preds = (probs > threshold).float()
        correct = (preds == y_true).float()
        acc = correct.mean().item()  # average over batch, time, and gates
    return acc

import matplotlib.pyplot as plt
def plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies, f1_accuracy):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    ax1.plot(train_losses, label="Train Loss")
    ax1.plot(val_losses, label="Val Loss")
    ax1.set_title("Loss over Epochs")
    ax1.legend()
    ax1.grid(True)

    ax2.plot(train_accuracies, label="Train Acc")
    ax2.plot(val_accuracies, label="Val Acc")
    ax2.plot(f1_accuracy,label="f1_accuracy")
    ax2.set_title("Accuracy over Epochs")
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.show()

# from sklearn.metrics import precision_recall_fscore_support, accuracy_score
# import numpy as np
# import torch

# def evaluate_metrics(all_trues, all_preds):
#     Flatten and ensure binary integer values
#     y_true = y_true.detach().cpu().view(-1).numpy()
#     y_pred = y_pred.detach().cpu().view(-1).numpy()

#     # Convert floats to integers (threshold already applied)
#     y_true = np.round(y_true).astype(int)
#     y_pred = np.round(y_pred).astype(int)

#     # Double-check binary values
#     assert set(np.unique(y_true)).issubset({0, 1}), f"y_true has unexpected values: {np.unique(y_true)}"
#     assert set(np.unique(y_pred)).issubset({0, 1}), f"y_pred has unexpected values: {np.unique(y_pred)}"

#     acc = accuracy_score(y_true, y_pred)
#     precision, recall, f1, _ = precision_recall_fscore_support(
#         y_true, y_pred, average='binary', zero_division=0
#     )

#     print(f"Accuracy: {acc:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1-score: {f1:.4f}")

from sklearn.metrics import precision_recall_fscore_support, accuracy_score
import numpy as np
import torch

from sklearn.metrics import precision_recall_fscore_support, accuracy_score

def evaluate_metrics(all_trues, all_preds):
    # Convert to numpy
    y_pred_np = torch.cat(all_preds).cpu().numpy().reshape(-1, 9)
    y_true_np = torch.cat(all_trues).cpu().numpy().reshape(-1, 9)

    # Ensure binary
    y_pred = np.round(y_pred_np).astype(int)
    y_true = np.round(y_true_np).astype(int)

    # Multi-label metrics
    acc = accuracy_score(y_true, y_pred)  # micro accuracy
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='micro', zero_division=0
    )

    print(f"Epoch Metrics → Accuracy: {acc:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1-score: {f1:.4f}")
    return acc, precision, recall, f1

import torch
from torch import nn, optim
import numpy as np
from sklearn.preprocessing import StandardScaler

# Set these as needed
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = TimeSeriesTransformer(num_features=112, d_model=32, nhead=4, num_layers=2, dropout=0.5,
                 context_len=60, target_len=30).to(device) #increasing dropout to 0.4
pos_weight = torch.tensor([4.8]).to(device)  #adding this to loss function will make misclassified 1 more expensive so the model will take 1 more seriously if its occurs less
loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weights_tensor)  # or BCEWithLogitsLoss depending on your target
optimizer = optim.Adam(model.parameters(), lr=1e-5 ,weight_decay = 1e-3)

num_epochs = 100
best_value = float('inf')
patience = 10
counter = 0
train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []
f1_accuracies = []

#normalising train data using custom method
# train_data = normlisation(train_data)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    total_acc = 0
    # How many batches per epoch? Pick based on dataset size or set manually
    for _ in range(100):  #  100 random batches per epoch
        x, y = get_batch("train")
        x, y = x.to(device), y.to(device)

        # Forward pass
        y_pred = model(x)

        # Compute loss
        loss = loss_fn(y_pred, y)

        acc = binary_accuracy(y_pred, y)


        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


        total_loss += loss.item()
        total_acc += acc

    avg_loss = total_loss / 100
    avg_acc = total_acc/100
    train_losses.append(avg_loss)
    train_accuracies.append(avg_acc)
    print(f"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}  Accuracy :{avg_acc}")
    print(f"y_pred shape: {y_pred.shape}")
    print(f"y shape: {y.shape}")
    # evaluate_metrics(y, y_pred)
    # Make sure these are detached from GPU and converted to NumPy




    # Optional: validation loop
    # model.eval()
    # with torch.no_grad():
    #     val_x, val_y = get_batch("val")
    #     val_x, val_y = val_x.to(device), val_y.to(device)
    #     val_pred = model(val_x)
    #     val_loss = loss_fn(val_pred, val_y)
    #     probs = torch.sigmoid(val_pred)
    #     preds = (probs > 0.5).float()

    #     val_acc = binary_accuracy(preds, val_y)
    #     val_accuracies.append(val_acc)
    #     print(f"          Val Loss: {val_loss.item():.4f}")
    #     val_losses.append(val_loss.item())
        # y_pred_np = preds.detach().cpu().numpy()
        # y_true_np = val_y.detach().cpu().numpy()

    #     # Optional: Flatten if needed
    #     y_pred_np = y_pred_np.reshape(-1)
    #     y_true_np = y_true_np.reshape(-1)
    model.eval()
    with torch.no_grad():
        val_total_loss = 0
        val_total_acc = 0
        val_steps = 20  # number of batches to validate on
        all_preds, all_trues = [], []


        for _ in range(val_steps):
            val_x, val_y = get_batch("val")
            val_x, val_y = val_x.to(device), val_y.to(device)

            val_pred = model(val_x)
            val_loss = loss_fn(val_pred, val_y)
            val_total_loss += val_loss.item()

            probs = torch.sigmoid(val_pred)
            preds = (probs > 0.5).float()

            val_total_acc += binary_accuracy(preds, val_y)
            all_preds.append(preds.detach().cpu())
            all_trues.append(val_y.detach().cpu())

            # Early stopping
            # if val_loss < best_value:
            #     best_value = val_loss
            #     counter = 0
            # else:
            #     counter += 1
            #     print(f"No improvent from last ,{counter}, epoch")
            # if counter >= patience:
            #     print("Early stopping triggered")
            #     break

            # Save for debug
            # all_preds.append(preds.detach().cpu())
            # all_trues.append(val_y.detach().cpu())
            # y_pred_np = preds.detach().cpu().numpy()
            # y_true_np = val_y.detach().cpu().numpy()
            # y_pred_np = torch.cat(all_preds).squeeze(-1).cpu().numpy().flatten()  # shape: [total_predictions]
            # y_true_np = torch.cat(all_trues).squeeze(-1).cpu().numpy().flatten()  # shape: [total_predictions]


            # # Print first 10 predictions and corresponding true labels
            # for i in range(10):
            #     print(f"Predicted: {y_pred_np[i]} , True: {y_true_np[i]}")


        avg_val_loss = val_total_loss / val_steps
        avg_val_acc = val_total_acc / val_steps
        print(f"          Val Loss: {avg_val_loss:.4f}")
        print(f"          Val Acc : {avg_val_acc:.4f}")
        eval_acc, precision, recall, f1_accuracy = evaluate_metrics(all_trues, all_preds)
        f1_accuracies.append(f1_accuracy)

        if avg_val_loss < best_value:
            best_value = avg_val_loss
            best_model_state = model.state_dict()
            print("Best Model Saved ")
            counter = 0
        else:
            counter += 1
            print(f"No improvent from last ,{counter}, epoch")
        if counter >= patience:
            print("Early stopping triggered")
            break

        val_losses.append(avg_val_loss)
        val_accuracies.append(avg_val_acc)



import torch
import torch.nn as nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.d_model = d_model

        # Create projection layers for Q, K, V
        self.query = nn.Linear(d_model, d_model)
        self.key   = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)

        # Optional: final output projection
        self.proj = nn.Linear(d_model, d_model)

    def forward(self, x):
        # x: [B, T, d_model]
        B, T, C = x.size()

        # Step 1: compute Q, K, V
        Q = self.query(x)  # [B, T, d_model]
        K = self.key(x)    # [B, T, d_model]
        V = self.value(x)  # [B, T, d_model]

        # Step 2: compute attention scores
        # Q @ K^T → [B, T, T]
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (C ** 0.5)

        # Step 3: apply softmax to get attention weights
        attn_weights = F.softmax(scores, dim=-1)  # [B, T, T]

        # Step 4: weighted sum of values means dot product(matrix multiplication) of attn_weights and values
        out = torch.matmul(attn_weights, V)  # [B, T, d_model]

        # Step 5: final projection (optional)
        out = self.proj(out)  # [B, T, d_model]

        return out

plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies, f1_accuracies)

torch.save(best_model_state, 'best_model_60.pth')
print("Model weights saved succesfully")

# import joblib
# joblib.dump(scaler, 'scaler.pkl')
# print("Scaler Normilisation saved successfully")

# test_data , num_features = load_time_series("/content/drive/MyDrive/datatest2.csv")
# test_data_normalised ,scaler = prepare_data(test_data,scaler = scaler, fit_scaler = True)
# model = TimeSeriesTransformer(
#     num_features=8,
#     d_model=32,
#     nhead=4,
#     num_layers=2,
#     dropout=0.5,
#     context_len=60,
#     target_len=30
# )
# model.load_state_dict(torch.load("best_model_60.pth"))  # or your saved file path
# all_preds = []
# all_trues = []
# val_steps = 100  # or set it based on dataset size
# device = 'cuda' if torch.cuda.is_available() else 'cpu'
# model.to(device)

# model.eval()
# with torch.no_grad():
#     for _ in range(val_steps):
#         x_test, y_test = get_batch("test")
#         x_test = x_test.to(device)
#         y_test = y_test.to(device)

#         y_pred = model(x_test)
#         probs = torch.sigmoid(y_pred)
#         preds = (probs > 0.5).float()

#         all_preds.append(preds.cpu())
#         all_trues.append(y_test.cpu())

# # Concatenate all predictions and ground truth
# y_pred_all = torch.cat(all_preds).numpy().flatten()
# y_true_all = torch.cat(all_trues).numpy().flatten()

# # Final metrics
# from sklearn.metrics import accuracy_score, f1_score
# print("Test Accuracy:", accuracy_score(y_true_all, y_pred_all))
# print("Test F1 Score:", f1_score(y_true_all, y_pred_all))
